{
  "tests/test-sampling.cpp": {
    "content": "#include \"ggml.h\"\n#include \"llama.h\"\n\n#ifdef NDEBUG\n#undef NDEBUG\n#endif\n\n#include <algorithm>\n#include <cmath>\n#include <string>\n#include <vector>\n\nextern struct llama_sampler * llama_sampler_init_dry_testing(int32_t context_size, float dry_multiplier, float dry_base, int32_t dry_allowed_length, int32_t dry_penalty_last_n, const std::vector<std::vector<llama_token>>& seq_breakers);\n\nstatic void dump(const llama_token_data_array * cur_p) {\n    for (size_t i = 0; i < cur_p->size; i++) {\n        printf(\"%d: %f (%f)\\n\", cur_p->data[i].id, cur_p->data[i].p, cur_p->data[i].logit);\n    }\n}\n\n#define DUMP(__cur_p) do { printf(\"%s:%d (%s)\\n\", __FILE__, __LINE__, __func__); dump((__cur_p)); printf(\"-\\n\"); } while(0)\n\nstruct sampler_tester {\n    sampler_tester(size_t n_vocab) {\n        cur.reserve(n_vocab);\n        for (llama_token token_id = 0; token_id < (llama_token)n_vocab; token_id++) {\n            const float logit = logf(token_id);\n            cur.emplace_back(llama_token_data{token_id, logit, 0.0f});\n        }\n\n        cur_p = llama_token_data_array { cur.data(), cur.size(), -1, false };\n    }\n\n    sampler_tester(const std::vector<float> & probs, const std::vector<float> & probs_expected) : probs_expected(probs_expected) {\n        cur.reserve(probs.size());\n        for (llama_token token_id = 0; token_id < (llama_token)probs.size(); token_id++) {\n            const float logit = logf(probs[token_id]);\n            cur.emplace_back(llama_token_data{token_id, logit, probs[token_id]});\n        }\n\n        cur_p = llama_token_data_array { cur.data(), cur.size(), -1, false };\n    }\n\n    void apply(llama_sampler * sampler) {\n        llama_sampler_apply(sampler, &cur_p);\n        llama_sampler_free(sampler);\n    }\n\n    void check() {\n        GGML_ASSERT(cur_p.size == probs_expected.size());\n        for (size_t i = 0; i < cur_p.size; i++) {\n            GGML_ASSERT(fabs(cur_p.data[i].p - probs_expected[i]) < 1e-5);\n        }\n    }\n\n    llama_token_data_array cur_p;\n\nprivate:\n    co",
    "language": "cpp"
  }
}