{
  "common/chat-parser.cpp": {
    "status": "modified",
    "patch": "@@ -55,7 +55,15 @@ bool common_chat_msg_parser::add_tool_call(const std::string & name, const std::\n bool common_chat_msg_parser::add_tool_call(const json & tool_call) {\n     std::string name = tool_call.contains(\"name\") ? tool_call.at(\"name\") : \"\";\n     std::string id = tool_call.contains(\"id\") ? tool_call.at(\"id\") : \"\";\n-    std::string arguments = tool_call.contains(\"arguments\") ? tool_call.at(\"arguments\") : \"\";\n+    std::string arguments = \"\";\n+    if (tool_call.contains(\"arguments\")) {\n+        if (tool_call.at(\"arguments\").is_object()) {\n+            arguments = tool_call.at(\"arguments\").dump();\n+        } else {\n+            arguments = tool_call.at(\"arguments\");\n+        }\n+    }\n+\n     return add_tool_call(name, id, arguments);\n }\n ",
    "additions": 9,
    "deletions": 1,
    "changes": 10,
    "language": "cpp",
    "imports": [
      "#include \"chat-parser.h\"",
      "#include \"common.h\"",
      "#include \"log.h\"",
      "#include \"regex-partial.h\"",
      "#include <optional>",
      "#include <stdexcept>",
      "#include <string>",
      "#include <vector>"
    ],
    "full_content": "#include \"chat-parser.h\"\n#include \"common.h\"\n#include \"log.h\"\n#include \"regex-partial.h\"\n\n#include <optional>\n#include <stdexcept>\n#include <string>\n#include <vector>\n\nusing json = nlohmann::ordered_json;\n\ncommon_chat_msg_parser::common_chat_msg_parser(const std::string & input, bool is_partial, const common_chat_syntax & syntax)\n    : input_(input), is_partial_(is_partial), syntax_(syntax)\n{\n    result_.role = \"assistant\";\n\n    while (true) {\n        std::string id = std::to_string(std::rand());\n        if (input.find(id) == std::string::npos) {\n            healing_marker_ = id;\n            break;\n        }\n    }\n}\n\nstd::string common_chat_msg_parser::str(const common_string_range & rng) const {\n    GGML_ASSERT(rng.begin <= rng.end);\n    return input_.substr(rng.begin, rng.end - rng.begin);\n}\n\nvoid common_chat_msg_parser::add_content(const std::string &content) {\n    result_.content += content;\n}\n\nvoid common_chat_msg_parser::add_reasoning_content(const std::string &reasoning_content) {\n    result_.reasoning_content += reasoning_content;\n}\n\nbool common_chat_msg_parser::add_tool_call(const std::string & name, const std::string & id, const std::string & arguments) {\n    if (name.empty()) {\n        return false;\n    }\n\n    common_chat_tool_call tool_call;\n    tool_call.name = name;\n    tool_call.arguments = arguments;\n    tool_call.id = id;\n\n    // LOG_DBG(\"Tool call arguments:\\n\\traw: %s\\n\\tresult: %s\\n\", arguments.c_str(), tool_call.arguments.c_str());\n    result_.tool_calls.emplace_back(tool_call);\n\n    return true;\n}\nbool common_chat_msg_parser::add_tool_call(const json & tool_call) {\n    std::string name = tool_call.contains(\"name\") ? tool_call.at(\"name\") : \"\";\n    std::string id = tool_call.contains(\"id\") ? tool_call.at(\"id\") : \"\";\n    std::string arguments = \"\";\n    if (tool_call.contains(\"arguments\")) {\n        if (tool_call.at(\"arguments\").is_object()) {\n            arguments = tool_call.at(\"arguments\").dump();\n        } else {\n            arguments = tool_ca",
    "raw_url": "https://github.com/ggml-org/llama.cpp/raw/e483e07bb35e6552806d56eee8b08daed516d551/common%2Fchat-parser.cpp",
    "is_test_file": false
  },
  "common/chat.cpp": {
    "status": "modified",
    "patch": "@@ -606,6 +606,7 @@ const char * common_chat_format_name(common_chat_format format) {\n         case COMMON_CHAT_FORMAT_FUNCTIONARY_V3_1_LLAMA_3_1: return \"Functionary v3.1 Llama 3.1\";\n         case COMMON_CHAT_FORMAT_HERMES_2_PRO: return \"Hermes 2 Pro\";\n         case COMMON_CHAT_FORMAT_COMMAND_R7B: return \"Command R7B\";\n+        case COMMON_CHAT_FORMAT_GRANITE: return \"Granite\";\n         case COMMON_CHAT_FORMAT_GPT_OSS: return \"GPT-OSS\";\n         default:\n             throw std::runtime_error(\"Unknown chat format\");\n@@ -618,6 +619,7 @@ const char * common_reasoning_format_name(common_reasoning_format format) {\n         case COMMON_REASONING_FORMAT_AUTO:     return \"auto\";\n         case COMMON_REASONING_FORMAT_DEEPSEEK: return \"deepseek\";\n         case COMMON_REASONING_FORMAT_DEEPSEEK_LEGACY: return \"deepseek-legacy\";\n+        case COMMON_REASONING_FORMAT_GRANITE: return \"granite\";\n         default:\n             throw std::runtime_error(\"Unknown reasoning format\");\n     }\n@@ -1734,6 +1736,124 @@ static void common_chat_parse_hermes_2_pro(common_chat_msg_parser & builder) {\n     builder.add_content(builder.consume_rest());\n }\n \n+static common_chat_params common_chat_params_init_granite(const common_chat_template & tmpl, const struct templates_params & inputs) {\n+    common_chat_params data;\n+\n+    // Pass thinking context for Granite template\n+    json additional_context = {\n+        {\"thinking\", inputs.enable_thinking},\n+    };\n+\n+    data.prompt = apply(tmpl, inputs, /* messages_override= */ std::nullopt, /* tools_override= */ std::nullopt, additional_context);\n+    data.format = COMMON_CHAT_FORMAT_GRANITE;\n+\n+    if (string_ends_with(data.prompt, \"<think>\\n\") || string_ends_with(data.prompt, \"<think>\")) {\n+        if (!inputs.enable_thinking) {\n+            data.prompt += \"</think>\";\n+        } else {\n+            data.thinking_forced_open = true;\n+        }\n+    }\n+\n+    if (!inputs.tools.is_null()) {\n+        // Granite uses <|tool_call|> followed by JSON list\n+        data.grammar_lazy = inputs.tool_choice != COMMON_CHAT_TOOL_CHOICE_REQUIRED;\n+        data.grammar = build_grammar([&](const common_grammar_builder & builder) {\n+            std::vector<std::string> tool_rules;\n+            foreach_function(inputs.tools, [&](const json & tool) {\n+                const auto & function = tool.at(\"function\");\n+                std::string name = function.at(\"name\");\n+                auto parameters = function.at(\"parameters\");\n+                builder.resolve_refs(parameters);\n+                tool_rules.push_back(builder.add_rule(name + \"-call\", builder.add_schema(name +\n+\"-args\", {\n+                    {\"type\", \"object\"},\n+                    {\"properties\", {\n+                        {\"name\", {{\"const\", name}}},\n+                        {\"arguments\", parameters},\n+                    }},\n+                    {\"required\", json::array({\"name\", \"arguments\"})},\n+                })));\n+            });\n+\n+            auto tool_call = builder.add_rule(\"tool_call\", string_join(tool_rules, \" | \"));\n+            auto tool_list = builder.add_rule(\"tool_list\", \"\\\"[\\\" space \" + tool_call + \" (\\\",\\\" space \" + tool_call + \")* space \\\"]\\\"\");\n+\n+            if (data.thinking_forced_open) {\n+                builder.add_rule(\"root\", \"\\\"</think>\\\" space \\\"<response>\\\" space [^<]* \\\"</response>\\\" space \\\"<|tool_call|>\\\" space \" + tool_list);\n+            } else {\n+                builder.add_rule(\"root\", \"\\\"<|tool_call|>\\\" space \" + tool_list);\n+            }\n+\n+            data.grammar_triggers.push_back({\n+                COMMON_GRAMMAR_TRIGGER_TYPE_WORD,\n+                \"<|tool_call|>\"\n+            });\n+\n+            data.preserved_tokens = {\n+                \"<think>\",\n+                \"</think>\",\n+                \"<response>\",\n+                \"</response>\",\n+                \"<|tool_call|>\",\n+            };\n+        });\n+    } else {\n+        // Handle thinking tags for non-tool responses\n+        if (data.thinking_forced_open && inputs.enable_thinking) {\n+            data.grammar_lazy = false;\n+            data.grammar = build_grammar([&](const common_grammar_builder & builder) {\n+                builder.add_rule(\"root\", \"\\\"</think>\\\" space \\\"<response>\\\" space .* \\\"</response>\\\" space\");\n+            });\n+            data.preserved_tokens = {\n+                \"<think>\",\n+                \"</think>\",\n+                \"<response>\",\n+                \"</response>\",\n+            };\n+        }\n+    }\n+\n+    return data;\n+}\n+\n+static void common_chat_parse_granite(common_chat_msg_parser & builder) {\n+    // Parse thinking tags\n+    builder.try_parse_reasoning(\"<think>\", \"</think>\");\n+\n+    // Parse response tags using regex\n+    static const common_regex response_regex(\"<response>([\\\\s\\\\S]*?)</response>\");\n+    if (auto res = builder.try_find_regex(response_regex)) {\n+        // Extract the content between the tags (capture group 1)\n+        auto content = builder.str(res->groups[1]);\n+        builder.add_content(content);\n+        builder.move_to(res->groups[0].end);\n+    }\n+\n+    if (!builder.syntax().parse_tool_calls) {\n+        builder.add_content(builder.consume_rest());\n+        return;\n+    }\n+\n+    // Look for tool calls\n+    static const common_regex tool_call_regex(regex_escape(\"<|tool_call|>\"));\n+    if (auto res = builder.try_find_regex(tool_call_regex)) {\n+        builder.move_to(res->groups[0].end);\n+\n+        // Expect JSON array of tool calls\n+        auto tool_calls_data = builder.consume_json();\n+        if (tool_calls_data.json.is_array()) {\n+            if (!builder.add_tool_calls(tool_calls_data.json)) {\n+                builder.add_content(\"<|tool_call|>\" + tool_calls_data.json.dump());\n+            }\n+        } else {\n+            builder.add_content(\"<|tool_call|>\" + tool_calls_data.json.dump());\n+        }\n+    } else {\n+        builder.add_content(builder.consume_rest());\n+    }\n+}\n+\n static common_chat_params common_chat_params_init_without_tools(const common_chat_template & tmpl, const struct templates_params & inputs) {\n     common_chat_params data;\n     data.prompt = apply(tmpl, inputs);\n@@ -1805,6 +1925,11 @@ static common_chat_params common_chat_templates_apply_jinja(\n         return common_chat_params_init_command_r7b(tmpl, params);\n     }\n \n+    // Granite (IBM) - detects thinking / tools support\n+    if (src.find(\"elif thinking\") != std::string::npos && src.find(\"<|tool_call|>\") != std::string::npos) {\n+        return common_chat_params_init_granite(tmpl, params);\n+    }\n+\n     // Hermes 2/3 Pro, Qwen 2.5 Instruct (w/ tools)\n     if (src.find(\"<tool_call>\") != std::string::npos && params.json_schema.is_null()) {\n         return common_chat_params_init_hermes_2_pro(tmpl, params);\n@@ -1865,6 +1990,7 @@ static common_chat_params common_chat_templates_apply_legacy(\n     int alloc_size = 0;\n     std::vector<llama_chat_message> chat;\n     std::vector<std::string> contents;\n+\n     for (const auto & msg : inputs.messages) {\n         auto content = msg.content;\n         for (const auto & part : msg.content_parts) {\n@@ -1966,6 +2092,9 @@ static void common_chat_parse(common_chat_msg_parser & builder) {\n         case COMMON_CHAT_FORMAT_COMMAND_R7B:\n             common_chat_parse_command_r7b(builder);\n             break;\n+        case COMMON_CHAT_FORMAT_GRANITE:\n+            common_chat_parse_granite(builder);\n+            break;\n         case COMMON_CHAT_FORMAT_GPT_OSS:\n             common_chat_parse_gpt_oss(builder);\n             break;",
    "additions": 129,
    "deletions": 0,
    "changes": 129,
    "language": "cpp",
    "imports": [
      "#include \"chat.h\"",
      "#include \"chat-parser.h\"",
      "#include \"common.h\"",
      "#include \"json-partial.h\"",
      "#include \"json-schema-to-grammar.h\"",
      "#include \"log.h\"",
      "#include \"regex-partial.h\"",
      "#include <minja/chat-template.hpp>",
      "#include <minja/minja.hpp>",
      "#include <cstdio>"
    ],
    "full_content": "#include \"chat.h\"\n#include \"chat-parser.h\"\n#include \"common.h\"\n#include \"json-partial.h\"\n#include \"json-schema-to-grammar.h\"\n#include \"log.h\"\n#include \"regex-partial.h\"\n\n#include <minja/chat-template.hpp>\n#include <minja/minja.hpp>\n\n#include <cstdio>\n#include <exception>\n#include <iostream>\n#include <optional>\n#include <stdexcept>\n#include <string>\n#include <vector>\n\nusing json = nlohmann::ordered_json;\n\nstatic std::string format_time(const std::chrono::system_clock::time_point & now, const std::string & format) {\n    auto time = std::chrono::system_clock::to_time_t(now);\n    auto local_time = *std::localtime(&time);\n    std::ostringstream ss;\n    ss << std::put_time(&local_time, format.c_str());\n    auto res = ss.str();\n    return res;\n}\n\nstatic std::string string_diff(const std::string & last, const std::string & current) {\n    if (last.empty()) {\n        return current;\n    }\n    if (!string_starts_with(current, last)) {\n        if (string_starts_with(last, current)) {\n            // This happens if the last generation ended on a partial stop word (not erased),\n            // and the current ended on a stop word (erased).\n            return \"\";\n        }\n        throw std::runtime_error(\"Invalid diff: '\" + last + \"' not found at start of '\" + current + \"'\");\n    }\n    return current.substr(last.size());\n}\n\nstatic bool has_content_or_tool_calls(const common_chat_msg & msg) {\n    return !msg.content.empty() || !msg.tool_calls.empty();\n}\n\ntemplate <>\njson common_chat_msg::to_json_oaicompat() const\n{\n    json message {\n        {\"role\", \"assistant\"},\n    };\n    if (!reasoning_content.empty()) {\n        message[\"reasoning_content\"] = reasoning_content;\n    }\n    if (content.empty() && !tool_calls.empty()) {\n        message[\"content\"] = json();\n    } else {\n        message[\"content\"] = content;\n    }\n    if (!tool_calls.empty()) {\n        auto arr = json::array();\n        for (const auto & tc : tool_calls) {\n            arr.push_back({\n                {\"type\", \"function",
    "raw_url": "https://github.com/ggml-org/llama.cpp/raw/e483e07bb35e6552806d56eee8b08daed516d551/common%2Fchat.cpp",
    "is_test_file": false
  },
  "common/chat.h": {
    "status": "modified",
    "patch": "@@ -109,6 +109,7 @@ enum common_chat_format {\n     COMMON_CHAT_FORMAT_FUNCTIONARY_V3_1_LLAMA_3_1,\n     COMMON_CHAT_FORMAT_HERMES_2_PRO,\n     COMMON_CHAT_FORMAT_COMMAND_R7B,\n+    COMMON_CHAT_FORMAT_GRANITE,\n     COMMON_CHAT_FORMAT_GPT_OSS,\n \n     COMMON_CHAT_FORMAT_COUNT, // Not a format, just the # formats",
    "additions": 1,
    "deletions": 0,
    "changes": 1,
    "language": "h",
    "imports": [],
    "full_content": "// Chat support (incl. tool call grammar constraining & output parsing) w/ generic & custom template handlers.\n\n#pragma once\n\n#include \"common.h\"\n#include <functional>\n#include <chrono>\n#include <string>\n#include <vector>\n#include <map>\n\nstruct common_chat_templates;\n\nstruct common_chat_tool_call {\n    std::string name;\n    std::string arguments;\n    std::string id;\n\n    bool operator==(const common_chat_tool_call & other) const {\n        return name == other.name && arguments == other.arguments && id == other.id;\n    }\n};\n\nstruct common_chat_msg_content_part {\n    std::string type;\n    std::string text;\n\n    bool operator==(const common_chat_msg_content_part & other) const {\n        return type == other.type && text == other.text;\n    }\n};\n\nstruct common_chat_msg {\n    std::string role;\n    std::string content;\n    std::vector<common_chat_msg_content_part> content_parts = {};\n    std::vector<common_chat_tool_call> tool_calls = {};\n    std::string reasoning_content;\n    std::string tool_name;\n    std::string tool_call_id;\n\n    template <class T> T to_json_oaicompat() const;\n\n    bool empty() const {\n        return content.empty() && content_parts.empty() && tool_calls.empty() && reasoning_content.empty() && tool_name.empty() && tool_call_id.empty();\n    }\n    void ensure_tool_call_ids_set(std::vector<std::string> & ids_cache, const std::function<std::string()> & gen_tool_call_id) {\n        for (auto i = 0u; i < tool_calls.size(); i++) {\n            if (ids_cache.size() <= i) {\n                auto id = tool_calls[i].id;\n                if (id.empty()) {\n                    id = gen_tool_call_id();\n                }\n                ids_cache.push_back(id);\n            }\n            tool_calls[i].id = ids_cache[i];\n        }\n    }\n    bool operator==(const common_chat_msg & other) const {\n        return role == other.role\n            && content == other.content\n            && content_parts == other.content_parts\n            && tool_calls == other.tool_calls\n          ",
    "raw_url": "https://github.com/ggml-org/llama.cpp/raw/e483e07bb35e6552806d56eee8b08daed516d551/common%2Fchat.h",
    "is_test_file": false
  },
  "common/common.h": {
    "status": "modified",
    "patch": "@@ -239,6 +239,7 @@ enum common_reasoning_format {\n     COMMON_REASONING_FORMAT_AUTO,\n     COMMON_REASONING_FORMAT_DEEPSEEK_LEGACY, // Extract thinking tag contents and return as `message.reasoning_content`, or leave inline in <think> tags in stream mode\n     COMMON_REASONING_FORMAT_DEEPSEEK,        // Extract thinking tag contents and return as `message.reasoning_content`, including in streaming deltas.\n+    COMMON_REASONING_FORMAT_GRANITE,         // Extract thinking tag contents and return as `message.reasoning_content`, including in streaming deltas.\n };\n \n struct common_params {",
    "additions": 1,
    "deletions": 0,
    "changes": 1,
    "language": "h",
    "imports": [],
    "full_content": "// Various helper functions and utilities\n\n#pragma once\n\n#include \"llama-cpp.h\"\n\n#include <set>\n#include <string>\n#include <string_view>\n#include <vector>\n#include <map>\n#include <sstream>\n\n#ifdef _WIN32\n#define DIRECTORY_SEPARATOR '\\\\'\n#else\n#define DIRECTORY_SEPARATOR '/'\n#endif // _WIN32\n\n#define die(msg)          do { fputs(\"error: \" msg \"\\n\", stderr);                exit(1); } while (0)\n#define die_fmt(fmt, ...) do { fprintf(stderr, \"error: \" fmt \"\\n\", __VA_ARGS__); exit(1); } while (0)\n\n#define print_build_info() do {                                                                     \\\n    fprintf(stderr, \"%s: build = %d (%s)\\n\",      __func__, LLAMA_BUILD_NUMBER, LLAMA_COMMIT);      \\\n    fprintf(stderr, \"%s: built with %s for %s\\n\", __func__, LLAMA_COMPILER, LLAMA_BUILD_TARGET);    \\\n} while(0)\n\n#define DEFAULT_MODEL_PATH \"models/7B/ggml-model-f16.gguf\"\n\nstruct common_adapter_lora_info {\n    std::string path;\n    float scale;\n\n    struct llama_adapter_lora * ptr;\n};\n\nusing llama_tokens = std::vector<llama_token>;\n\n// build info\nextern int LLAMA_BUILD_NUMBER;\nextern const char * LLAMA_COMMIT;\nextern const char * LLAMA_COMPILER;\nextern const char * LLAMA_BUILD_TARGET;\n\nstruct common_control_vector_load_info;\n\n//\n// CPU utils\n//\n\nstruct cpu_params {\n    int      n_threads                   = -1;\n    bool     cpumask[GGML_MAX_N_THREADS] = {false}; // CPU affinity mask.\n    bool     mask_valid                  = false;   // Default: any CPU\n    enum ggml_sched_priority  priority   = GGML_SCHED_PRIO_NORMAL;  // Scheduling prio : (0 - normal, 1 - medium, 2 - high, 3 - realtime)\n    bool     strict_cpu                  = false;   // Use strict CPU placement\n    uint32_t poll                        = 50;      // Polling (busywait) level (0 - no polling, 100 - mostly polling)\n};\n\nint32_t cpu_get_num_physical_cores();\nint32_t cpu_get_num_math();\n\n//\n// Common params\n//\n\nenum llama_example {\n    LLAMA_EXAMPLE_COMMON,\n    LLAMA_EXAMPLE_SPECULATIVE,\n    LLAMA_EXAMPLE_MAI",
    "raw_url": "https://github.com/ggml-org/llama.cpp/raw/e483e07bb35e6552806d56eee8b08daed516d551/common%2Fcommon.h",
    "is_test_file": false
  },
  "models/templates/ibm-granite-granite-3.3-2B-Instruct.jinja": {
    "status": "added",
    "patch": "@@ -0,0 +1,59 @@\n+{# Alias tools -> available_tools #}\n+{%- if tools and not available_tools -%}\n+    {%- set available_tools = tools -%}\n+{%- endif -%}\n+{%- if messages[0]['role'] == 'system' %}\n+     {%- set system_message = messages[0]['content'] %}\n+     {%- set loop_messages = messages[1:] %}\n+ {%- else %}\n+     {%- set system_message = \"Knowledge Cutoff Date: April 2024. Today's Date: \" + strftime_now('%B %d, %Y') + \". You are Granite, developed by IBM.\" %}\n+     {%- if available_tools and documents %}\n+         {%- set system_message = system_message + \" You are a helpful assistant with access to the following tools. When a tool is required to answer the user's query, respond only with <|tool_call|> followed by a JSON list of tools used. If a tool does not exist in the provided list of tools, notify the user that you do not have the ability to fulfill the request. Write the response to the user's input by strictly aligning with the facts in the provided documents. If the information needed to answer the question is not available in the documents, inform the user that the question cannot be answered based on the available data.\" %}\n+     {%- elif available_tools %}\n+         {%- set system_message = system_message + \" You are a helpful assistant with access to the following tools. When a tool is required to answer the user's query, respond only with <|tool_call|> followed by a JSON list of tools used. If a tool does not exist in the provided list of tools, notify the user that you do not have the ability to fulfill the request.\" %}\n+     {%- elif documents %}\n+         {%- set system_message = system_message + \" Write the response to the user's input by strictly aligning with the facts in the provided documents. If the information needed to answer the question is not available in the documents, inform the user that the question cannot be answered based on the available data.\" %}\n+    {%- elif thinking %}\n+    {%- set system_message = system_message + \" You are a helpful AI assistant.\n+Respond to every user query in a comprehensive and detailed way. You can write down your thoughts and reasoning process before responding. In the thought process, engage in a comprehensive cycle of analysis, summarization, exploration, reassessment, reflection, backtracing, and iteration to develop well-considered thinking process. In the response section, based on various attempts, explorations, and reflections from the thoughts section, systematically present the final solution that you deem correct. The response should summarize the thought process. Write your thoughts between <think></think> and write your response between <response></response> for each user query.\" %}\n+     {%- else %}\n+         {%- set system_message = system_message + \" You are a helpful AI assistant.\" %}\n+     {%- endif %}\n+     {%- if 'citations' in controls and documents %}\n+         {%- set system_message = system_message + '\n+Use the symbols <|start_of_cite|> and <|end_of_cite|> to indicate when a fact comes from a document in the search result, e.g <|start_of_cite|> {document_id: 1}my fact <|end_of_cite|> for a fact from document 1. Afterwards, list all the citations with their corresponding documents in an ordered list.' %}\n+     {%- endif %}\n+     {%- if 'hallucinations' in controls and documents %}\n+         {%- set system_message = system_message + '\n+Finally, after the response is written, include a numbered list of sentences from the response with a corresponding risk value that are hallucinated and not based in the documents.' %}\n+     {%- endif %}\n+     {%- set loop_messages = messages %}\n+ {%- endif %}\n+ {{- '<|start_of_role|>system<|end_of_role|>' + system_message + '<|end_of_text|>\n+' }}\n+ {%- if available_tools %}\n+     {{- '<|start_of_role|>available_tools<|end_of_role|>' }}\n+     {{- available_tools | tojson(indent=4) }}\n+     {{- '<|end_of_text|>\n+' }}\n+ {%- endif %}\n+ {%- if documents %}\n+     {%- for document in documents %}\n+         {{- '<|start_of_role|>document {\"document_id\": \"' + document['doc_id'] | string + '\"}<|end_of_role|>\n+' }}\n+         {{- document['text'] }}\n+         {{- '<|end_of_text|>\n+' }}\n+              {%- endfor %}\n+ {%- endif %}\n+ {%- for message in loop_messages %}\n+     {{- '<|start_of_role|>' + message['role'] + '<|end_of_role|>' + message['content'] + '<|end_of_text|>\n+' }}\n+     {%- if loop.last and add_generation_prompt %}\n+         {{- '<|start_of_role|>assistant' }}\n+             {%- if controls %}\n+                 {{- ' ' + controls | tojson()}}\n+             {%- endif %}\n+         {{- '<|end_of_role|>' }}\n+     {%- endif %}\n+ {%- endfor %}",
    "additions": 59,
    "deletions": 0,
    "changes": 59,
    "language": "jinja",
    "imports": [],
    "full_content": "{# Alias tools -> available_tools #}\n{%- if tools and not available_tools -%}\n    {%- set available_tools = tools -%}\n{%- endif -%}\n{%- if messages[0]['role'] == 'system' %}\n     {%- set system_message = messages[0]['content'] %}\n     {%- set loop_messages = messages[1:] %}\n {%- else %}\n     {%- set system_message = \"Knowledge Cutoff Date: April 2024. Today's Date: \" + strftime_now('%B %d, %Y') + \". You are Granite, developed by IBM.\" %}\n     {%- if available_tools and documents %}\n         {%- set system_message = system_message + \" You are a helpful assistant with access to the following tools. When a tool is required to answer the user's query, respond only with <|tool_call|> followed by a JSON list of tools used. If a tool does not exist in the provided list of tools, notify the user that you do not have the ability to fulfill the request. Write the response to the user's input by strictly aligning with the facts in the provided documents. If the information needed to answer the question is not available in the documents, inform the user that the question cannot be answered based on the available data.\" %}\n     {%- elif available_tools %}\n         {%- set system_message = system_message + \" You are a helpful assistant with access to the following tools. When a tool is required to answer the user's query, respond only with <|tool_call|> followed by a JSON list of tools used. If a tool does not exist in the provided list of tools, notify the user that you do not have the ability to fulfill the request.\" %}\n     {%- elif documents %}\n         {%- set system_message = system_message + \" Write the response to the user's input by strictly aligning with the facts in the provided documents. If the information needed to answer the question is not available in the documents, inform the user that the question cannot be answered based on the available data.\" %}\n    {%- elif thinking %}\n    {%- set system_message = system_message + \" You are a helpful AI assistant.\nRespond t",
    "raw_url": "https://github.com/ggml-org/llama.cpp/raw/e483e07bb35e6552806d56eee8b08daed516d551/models%2Ftemplates%2Fibm-granite-granite-3.3-2B-Instruct.jinja",
    "is_test_file": false
  },
  "tests/test-chat.cpp": {
    "status": "modified",
    "patch": "@@ -1386,6 +1386,59 @@ static void test_template_output_parsers() {\n                 \"{\\\"arg1\\\": 1}\\n\"\n                 \"```<\uff5ctool\u2581call\u2581end\uff5c><\uff5ctool\u2581calls\u2581end\uff5c>\");\n     }\n+    {\n+        auto tmpls = read_templates(\"models/templates/ibm-granite-granite-3.3-2B-Instruct.jinja\");\n+        std::vector<std::string> end_tokens{ \"<|end_of_text|>\" };\n+\n+        assert_equals(COMMON_CHAT_FORMAT_GRANITE, common_chat_templates_apply(tmpls.get(), inputs_no_tools).format);\n+\n+        assert_equals(COMMON_CHAT_FORMAT_GRANITE, common_chat_templates_apply(tmpls.get(), inputs_tools).format);\n+\n+        // Test parsing regular content\n+        assert_msg_equals(message_assist,\n+            common_chat_parse(\n+                \"Hello, world!\\nWhat's up?\",\n+                /* is_partial= */ false,\n+                {COMMON_CHAT_FORMAT_GRANITE}));\n+\n+        // Test parsing content with thinking\n+        assert_msg_equals(message_assist_thoughts,\n+            common_chat_parse(\n+                \"<think>I'm\\nthinking</think>Hello, world!\\nWhat's up?\",\n+                /* is_partial= */ false,\n+                {\n+                    /* .format = */ COMMON_CHAT_FORMAT_GRANITE,\n+                    /* .reasoning_format = */ COMMON_REASONING_FORMAT_GRANITE,\n+                }));\n+\n+        // Test parsing tool calls\n+        assert_msg_equals(message_assist_call,\n+            common_chat_parse(\n+                \"<|tool_call|>[{\\\"name\\\": \\\"special_function\\\", \\\"arguments\\\": {\\\"arg1\\\": 1}}]\",\n+                /* is_partial= */ false,\n+                {COMMON_CHAT_FORMAT_GRANITE}));\n+\n+        // Test template generation for regular content\n+        test_templates(tmpls.get(), end_tokens, message_assist, tools,\n+                      \"Hello, world!\\nWhat's up?\",\n+                      /* expect_grammar_triggered= */ false);\n+\n+        // Test template generation for tool calls\n+        test_templates(tmpls.get(), end_tokens, message_assist_call_id, tools,\n+                      \"{\\n\"\n+                      \"  \\\"tool_calls\\\": [\\n\"\n+                      \"    {\\n\"\n+                      \"      \\\"name\\\": \\\"special_function\\\",\\n\"\n+                      \"      \\\"arguments\\\": {\\n\"\n+                      \"        \\\"arg1\\\": 1\\n\"\n+                      \"      },\\n\"\n+                      \"      \\\"id\\\": \\\"123456789\\\"\\n\"\n+                      \"    }\\n\"\n+                      \"  ]\\n\"\n+                      \"}\",\n+                      /* expect_grammar_triggered= */ false\n+        );\n+    }\n }\n \n static void test_msg_diffs_compute() {",
    "additions": 53,
    "deletions": 0,
    "changes": 53,
    "language": "cpp",
    "imports": [
      "#include \"chat.h\"",
      "#include \"log.h\"",
      "#include \"../src/unicode.h\"",
      "#include \"../src/llama-grammar.h\"",
      "#include <nlohmann/json.hpp>",
      "#include <fstream>",
      "#include <iostream>",
      "#include <string>"
    ],
    "full_content": "//  Tests chat handling, including grammar generation and parsing for tool calling, for various templates.\n//\n//  Also acts as a CLI to generate a Markdown summary of the formats of Jinja templates,\n//  e.g. given Minja (http://github.com/google/minja) checked out in parent dir:\n//\n//    cmake -B build && cmake --build build --parallel && ./build/bin/test-chat ../minja/build/tests/*.jinja 2>/dev/null\n//\n#include \"chat.h\"\n\n#include \"log.h\"\n\n#include \"../src/unicode.h\"\n#include \"../src/llama-grammar.h\"\n\n#include <nlohmann/json.hpp>\n\n#include <fstream>\n#include <iostream>\n#include <string>\n\nusing json = nlohmann::ordered_json;\n\nstatic std::ostream & operator<<(std::ostream & os, const common_chat_msg_diff & diff) {\n    os << \"{ content_delta: \" << diff.content_delta << \"; \";\n    os << \"reasoning_content_delta: \" << diff.reasoning_content_delta << \"; \";\n    if (diff.tool_call_index != std::string::npos) {\n        os << \"tool_call_index: \" << diff.tool_call_index << \"; \";\n        os << \"tool_call_delta.name: \" << diff.tool_call_delta.name << \"; \";\n        os << \"tool_call_delta.id: \" << diff.tool_call_delta.id << \"; \";\n        os << \"tool_call_delta.arguments: \" << diff.tool_call_delta.arguments << \"; \";\n    }\n    os << \"}\";\n    return os;\n}\n// operator<< for vector<common_chat_msg_diff>:\nstatic std::ostream & operator<<(std::ostream & os, const std::vector<common_chat_msg_diff> & diffs) {\n    os << \"[\\n\";\n    for (const auto & diff : diffs) {\n        os << \"  \" << diff << \",\\n\";\n    }\n    os << \"]\";\n    return os;\n}\nstatic std::ostream & operator<<(std::ostream & os, const common_chat_msg & msg) {\n    os << \"{ role: \" << msg.role << \"; \";\n    os << \"content: \" << msg.content << \"; \";\n    os << \"content_parts: [\\n\";\n    for (const auto & part : msg.content_parts) {\n        os << \"  { type: \" << part.type << \"; text: \" << part.text << \" },\\n\";\n    }\n    os << \"]; \";\n    os << \"reasoning_content: \" << msg.reasoning_content << \"; \";\n    os << \"tool_calls: [\\n\";\n    for (con",
    "raw_url": "https://github.com/ggml-org/llama.cpp/raw/e483e07bb35e6552806d56eee8b08daed516d551/tests%2Ftest-chat.cpp",
    "is_test_file": true
  }
}