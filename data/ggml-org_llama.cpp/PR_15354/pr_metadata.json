{
  "title": "vulkan: Optimize argsort",
  "description": "- Launch an appropriate number of invocations (next larger power of two). 32 invocations is common and the barrier is much cheaper there.\r\n- Specialize for \"needs bounds checking\" vs not.\r\n- Make the code less branchy and [[unroll]] the loops. In the final code, I see no branches inside the main loop (only predicated stores) when needs_bounds_check is false.\r\n- Always sort ascending, then apply the ascending vs descending option when doing the final stores to memory.\r\n- Copy the values into shared memory, makes them slightly cheaper to access.\r\n\r\nargsort is used in MoE models and I've seen it consuming as much as ~6% of the time on 5090.\r\n\r\n```\r\n5090 before\r\n\r\nllama-bench.exe -fa 1 -n 128 -p 512 -r 5 --prio 1 -m c:\\models\\Qwen_Qwen3-30B-A3B-Q2_K.gguf -m c:\\models\\\\deepseek-v2-lite-safetensors\\deepseek-v2-lite-Q4_K_M.gguf -m c:\\models\\gpt-oss-20b-mxfp4.gguf\r\nggml_vulkan: Found 1 Vulkan devices:\r\nggml_vulkan: 0 = NVIDIA GeForce RTX 5090 (NVIDIA) | uma: 0 | fp16: 1 | bf16: 1 | warp size: 32 | shared memory: 49152 | int dot: 1 | matrix cores: NV_coopmat2\r\n| model                          |       size |     params | backend    | ngl | fa |            test |                  t/s |\r\n| ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: |\r\n| qwen3moe 30B.A3B Q2_K - Medium |  10.15 GiB |    30.53 B | Vulkan     |  99 |  1 |           pp512 |      3692.93 \u00b1 35.46 |\r\n| qwen3moe 30B.A3B Q2_K - Medium |  10.15 GiB |    30.53 B | Vulkan     |  99 |  1 |           tg128 |        178.74 \u00b1 1.05 |\r\n| deepseek2 16B Q4_K - Medium    |   9.65 GiB |    15.71 B | Vulkan     |  99 |  1 |           pp512 |     6963.86 \u00b1 151.75 |\r\n| deepseek2 16B Q4_K - Medium    |   9.65 GiB |    15.71 B | Vulkan     |  99 |  1 |           tg128 |        233.44 \u00b1 1.56 |\r\n| gpt-oss ?B MXFP4 MoE           |  11.27 GiB |    20.91 B | Vulkan     |  99 |  1 |           pp512 |     6309.57 \u00b1 212.52 |\r\n| gpt-oss ?B MXFP4 MoE           |  11.27 GiB |    20.91 B | Vulkan     |  99 |  1 |           tg128 |        213.15 \u00b1 0.66 |\r\n\r\n5090 after\r\n\r\nggml_vulkan: 0 = NVIDIA GeForce RTX 5090 (NVIDIA) | uma: 0 | fp16: 1 | bf16: 1 | warp size: 32 | shared memory: 49152 | int dot: 1 | matrix cores: NV_coopmat2\r\n| model                          |       size |     params | backend    | ngl | fa |            test |                  t/s |\r\n| ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: |\r\n| qwen3moe 30B.A3B Q2_K - Medium |  10.15 GiB |    30.53 B | Vulkan     |  99 |  1 |           pp512 |      3705.48 \u00b1 27.75 |\r\n| qwen3moe 30B.A3B Q2_K - Medium |  10.15 GiB |    30.53 B | Vulkan     |  99 |  1 |           tg128 |        187.46 \u00b1 0.70 |\r\n| deepseek2 16B Q4_K - Medium    |   9.65 GiB |    15.71 B | Vulkan     |  99 |  1 |           pp512 |     7031.33 \u00b1 107.40 |\r\n| deepseek2 16B Q4_K - Medium    |   9.65 GiB |    15.71 B | Vulkan     |  99 |  1 |           tg128 |        239.63 \u00b1 1.29 |\r\n| gpt-oss ?B MXFP4 MoE           |  11.27 GiB |    20.91 B | Vulkan     |  99 |  1 |           pp512 |      6451.10 \u00b1 67.40 |\r\n| gpt-oss ?B MXFP4 MoE           |  11.27 GiB |    20.91 B | Vulkan     |  99 |  1 |           tg128 |        215.31 \u00b1 2.27 |\r\n\r\n4070 before\r\n\r\nggml_vulkan: 0 = NVIDIA GeForce RTX 4070 (NVIDIA) | uma: 0 | fp16: 1 | bf16: 1 | warp size: 32 | shared memory: 49152 | int dot: 1 | matrix cores: NV_coopmat2\r\n| model                          |       size |     params | backend    | ngl | fa |            test |                  t/s |\r\n| ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: |\r\n| qwen3moe 30B.A3B Q2_K - Medium |  10.15 GiB |    30.53 B | Vulkan     |  99 |  1 |           pp512 |      1429.72 \u00b1 10.05 |\r\n| qwen3moe 30B.A3B Q2_K - Medium |  10.15 GiB |    30.53 B | Vulkan     |  99 |  1 |           tg128 |        134.21 \u00b1 0.30 |\r\n| deepseek2 16B Q4_K - Medium    |   9.65 GiB |    15.71 B | Vulkan     |  99 |  1 |           pp512 |       1980.77 \u00b1 5.35 |\r\n| deepseek2 16B Q4_K - Medium    |   9.65 GiB |    15.71 B | Vulkan     |  99 |  1 |           tg128 |        162.06 \u00b1 0.54 |\r\n| gpt-oss ?B MXFP4 MoE           |  11.27 GiB |    20.91 B | Vulkan     |  99 |  1 |           pp512 |      2565.50 \u00b1 21.49 |\r\n| gpt-oss ?B MXFP4 MoE           |  11.27 GiB |    20.91 B | Vulkan     |  99 |  1 |           tg128 |        120.41 \u00b1 0.14 |\r\n\r\n4070 after\r\n\r\nggml_vulkan: 0 = NVIDIA GeForce RTX 4070 (NVIDIA) | uma: 0 | fp16: 1 | bf16: 1 | warp size: 32 | shared memory: 49152 | int dot: 1 | matrix cores: NV_coopmat2\r\n| model                          |       size |     params | backend    | ngl | fa |            test |                  t/s |\r\n| ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: |\r\n| qwen3moe 30B.A3B Q2_K - Medium |  10.15 GiB |    30.53 B | Vulkan     |  99 |  1 |           pp512 |      1445.78 \u00b1 10.08 |\r\n| qwen3moe 30B.A3B Q2_K - Medium |  10.15 GiB |    30.53 B | Vulkan     |  99 |  1 |           tg128 |        136.71 \u00b1 0.38 |\r\n| deepseek2 16B Q4_K - Medium    |   9.65 GiB |    15.71 B | Vulkan     |  99 |  1 |           pp512 |       1991.46 \u00b1 7.27 |\r\n| deepseek2 16B Q4_K - Medium    |   9.65 GiB |    15.71 B | Vulkan     |  99 |  1 |           tg128 |        162.19 \u00b1 0.83 |\r\n| gpt-oss ?B MXFP4 MoE           |  11.27 GiB |    20.91 B | Vulkan     |  99 |  1 |           pp512 |      2571.92 \u00b1 13.14 |\r\n| gpt-oss ?B MXFP4 MoE           |  11.27 GiB |    20.91 B | Vulkan     |  99 |  1 |           tg128 |        120.03 \u00b1 0.20 |\r\n```\r\n\r\n\r\n",
  "base_branch": "master",
  "head_branch": "argsort"
}